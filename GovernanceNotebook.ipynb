{"cells":[{"cell_type":"code","source":["# ================================\n","# POWER BI ENVIRONMENT DETAIL EXTRACTOR\n","# ================================\n","# \n","# This notebook extracts comprehensive Power BI environment metadata\n","# using the Fabric sempy library and REST APIs, mimicking the PowerShell\n","# script from:\n","# https://github.com/chris1642/Power-BI-Backup-Impact-Analysis-Governance-Solution\n","#\n","# EXTRACTED DATA (written to lakehouse tables):\n","# 1. Workspaces - workspace metadata with renamed columns\n","# 2. FabricItems - Fabric items (excluding Reports and SemanticModels)\n","# 3. Datasets - dataset metadata with renamed columns\n","# 4. DatasetSourcesInfo - dataset data sources\n","# 5. DatasetRefreshHistory - dataset refresh history\n","# 6. Dataflows - dataflow metadata with renamed columns\n","# 7. DataflowLineage - dataflow lineage (upstream dataflows)\n","# 8. DataflowSourcesInfo - dataflow data sources\n","# 9. DataflowRefreshHistory - dataflow refresh history\n","# 10. Reports - report metadata with renamed columns\n","# 11. ReportPages - report pages with renamed columns\n","# 12. Apps - Power BI apps\n","# 13. AppReports - reports within apps\n","#\n","# All column names are renamed to match the PowerShell script output.\n","# ================================\n","\n","# %pip install semantic-link-labs --quiet\n","\n","import time\n","import re\n","import pandas as pd\n","import json\n","from datetime import datetime\n","import sempy.fabric as fabric\n","from sempy.fabric import FabricRestClient\n","\n","# -----------------------------------\n","# CONFIG\n","# -----------------------------------\n","LAKEHOUSE_NAME = \"dbo\"          # <-- CHANGE THIS\n","SINGLE_WORKSPACE_NAME = None   # <-- or set to None to scan all\n","\n","# Validate lakehouse name\n","if not LAKEHOUSE_NAME:\n","    raise ValueError(\"LAKEHOUSE_NAME must be set! Please provide a valid lakehouse name (alphanumeric and underscores only).\")\n","    \n","if not re.match(r'^[a-zA-Z0-9_]+$', LAKEHOUSE_NAME):\n","    raise ValueError(f\"Invalid lakehouse name: '{LAKEHOUSE_NAME}'. Must contain only alphanumeric characters and underscores.\")\n","\n","EXTRACTION_TIMESTAMP = datetime.now()\n","REPORT_DATE = EXTRACTION_TIMESTAMP.strftime(\"%Y-%m-%d\")\n","start_time = time.time()\n","\n","# -----------------------------------\n","# Logging helpers\n","# -----------------------------------\n","def log(msg):\n","    print(msg, flush=True)\n","\n","def elapsed_min():\n","    return (time.time() - start_time) / 60\n","\n","# Heartbeat\n","import threading\n","heartbeat_running = True\n","def heartbeat():\n","    while heartbeat_running:\n","        time.sleep(10)\n","        print(f\"[Heartbeat] Still running… elapsed {elapsed_min():.2f} min\", flush=True)\n","\n","threading.Thread(target=heartbeat, daemon=True).start()\n","\n","# -----------------------------------\n","# Start banner\n","# -----------------------------------\n","log(\"=\"*80)\n","log(\"POWER BI ENVIRONMENT DETAIL EXTRACTION\")\n","log(f\"Started: {EXTRACTION_TIMESTAMP}\")\n","log(\"=\"*80)\n","\n","# ============================================\n","# AUTO-CREATE SCHEMA (LAKEHOUSE)\n","# ============================================\n","CATALOG = spark.sql(\"SELECT current_catalog()\").first()[0]\n","log(f\"Using catalog: {CATALOG}\")\n","\n","schema_name = f\"{CATALOG}.{LAKEHOUSE_NAME}\"\n","log(f\"Ensuring lakehouse schema exists: {schema_name}\")\n","\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n","log(f\"✓ Schema is ready: {schema_name}\\n\")\n","\n","\n","# ==============================================================  \n","# COLLECTIONS - Matching PowerShell script structure\n","# ==============================================================\n","\n","workspaces_info = []\n","fabric_items_info = []\n","datasets_info = []\n","dataset_sources_info = []\n","dataset_refresh_history = []\n","dataflows_info = []\n","dataflow_lineage = []\n","dataflow_sources_info = []\n","dataflow_refresh_history = []\n","reports_info = []\n","report_pages_info = []\n","apps_info = []\n","reports_in_app_info = []\n","\n","# Lookup tables\n","dataset_name_lookup = {}\n","dataflow_name_lookup = {}\n","\n","# ==============================================================  \n","# SAMPLE ROWS FOR EMPTY TABLE CREATION\n","# ==============================================================\n","\n","SAMPLE_ROWS = {\n","    \"Workspaces\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"WorkspaceType\": \"\", \"WorkspaceCapacityId\": \"\"},\n","    \"FabricItems\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"FabricItemID\": \"\", \"FabricItemType\": \"\", \"FabricItemName\": \"\", \"FabricItemDescription\": \"\"},\n","    \"Datasets\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DatasetId\": \"\", \"DatasetName\": \"\", \"DatasetDescription\": \"\", \"DatasetWebUrl\": \"\", \"DatasetConfiguredBy\": \"\", \"DatasetIsRefreshable\": False, \"DatasetTargetStorageMode\": \"\", \"DatasetCreatedDate\": \"\"},\n","    \"DatasetSourcesInfo\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DatasetId\": \"\", \"DatasetName\": \"\", \"DatasetDatasourceType\": \"\", \"DatasetDatasourceId\": \"\", \"DatasetDatasourceGatewayId\": \"\", \"DatasetDatasourceConnectionDetails\": \"\"},\n","    \"DatasetRefreshHistory\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DatasetId\": \"\", \"DatasetName\": \"\", \"DatasetRefreshRequestId\": \"\", \"DatasetRefreshId\": \"\", \"DatasetRefreshStartTime\": \"\", \"DatasetRefreshEndTime\": \"\", \"DatasetRefreshStatus\": \"\", \"DatasetRefreshType\": \"\"},\n","    \"Dataflows\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DataflowId\": \"\", \"DataflowName\": \"\", \"DataflowDescription\": \"\", \"DataflowConfiguredBy\": \"\", \"DataflowModifiedBy\": \"\", \"DataflowModifiedDateTime\": \"\", \"DataflowJsonURL\": \"\", \"DataflowGeneration\": \"\"},\n","    \"DataflowLineage\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DataflowId\": \"\", \"DataflowName\": \"\", \"DatasetId\": \"\", \"DatasetName\": \"\"},\n","    \"DataflowSourcesInfo\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DataflowId\": \"\", \"DataflowName\": \"\", \"DataflowDatasourceType\": \"\", \"DataflowDatasourceId\": \"\", \"DataflowDatasourceGatewayId\": \"\", \"DataflowDatasourceConnectionDetails\": \"\"},\n","    \"DataflowRefreshHistory\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"DataflowId\": \"\", \"DataflowName\": \"\", \"DataflowRefreshRequestId\": \"\", \"DataflowRefreshId\": \"\", \"DataflowRefreshStartTime\": \"\", \"DataflowRefreshEndTime\": \"\", \"DataflowRefreshStatus\": \"\", \"DataflowRefreshType\": \"\", \"DataflowErrorInfo\": \"\"},\n","    \"Reports\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"ReportId\": \"\", \"ReportName\": \"\", \"ReportDescription\": \"\", \"ReportWebUrl\": \"\", \"ReportEmbedUrl\": \"\", \"ReportType\": \"\", \"DatasetId\": \"\", \"DatasetName\": \"\"},\n","    \"ReportPages\": {\"WorkspaceId\": \"\", \"WorkspaceName\": \"\", \"ReportId\": \"\", \"ReportName\": \"\", \"PageName\": \"\", \"PageDisplayName\": \"\", \"PageOrder\": 0},\n","    \"Apps\": {\"AppId\": \"\", \"AppName\": \"\", \"AppLastUpdate\": \"\", \"AppDescription\": \"\", \"AppPublishedBy\": \"\", \"AppWorkspaceId\": \"\", \"WorkspaceName\": \"\"},\n","    \"AppReports\": {\"AppId\": \"\", \"AppName\": \"\", \"AppReportId\": \"\", \"AppReportType\": \"\", \"ReportName\": \"\", \"AppReportWebUrl\": \"\", \"AppReportEmbedUrl\": \"\", \"AppReportIsOwnedByMe\": False, \"AppReportDatasetId\": \"\", \"ReportId\": \"\", \"WorkspaceName\": \"\"}\n","}\n","\n","# ==============================================================  \n","# HELPER FUNCTIONS\n","# ==============================================================\n","\n","def safe_get(row, column, default=\"\"):\n","    \"\"\"Safely get value from row\"\"\"\n","    try:\n","        val = row.get(column, default)\n","        return val if val is not None else default\n","    except Exception:\n","        return default\n","\n","def serialize_json(obj):\n","    \"\"\"Serialize object to JSON if non-empty, otherwise return empty string\"\"\"\n","    if obj:\n","        return json.dumps(obj)\n","    return \"\"\n","\n","# ==============================================================  \n","# GET WORKSPACES\n","# ==============================================================\n","\n","log(\"Fetching workspaces...\")\n","workspaces_df = fabric.list_workspaces()\n","\n","if SINGLE_WORKSPACE_NAME:\n","    workspaces_df = workspaces_df[workspaces_df[\"Name\"] == SINGLE_WORKSPACE_NAME]\n","    if workspaces_df.empty:\n","        raise ValueError(f\"Workspace '{SINGLE_WORKSPACE_NAME}' not found.\")\n","    log(f\"Filtering to workspace: {SINGLE_WORKSPACE_NAME}\")\n","\n","log(f\"Workspace count: {len(workspaces_df)}\")\n","\n","# Build workspaces_info with renamed columns\n","for _, ws_row in workspaces_df.iterrows():\n","    workspaces_info.append({\n","        \"WorkspaceId\": safe_get(ws_row, \"Id\"),\n","        \"WorkspaceName\": safe_get(ws_row, \"Name\"),\n","        \"WorkspaceType\": safe_get(ws_row, \"Type\"),\n","        \"WorkspaceCapacityId\": safe_get(ws_row, \"Capacity Id\")\n","    })\n","\n","log(f\"✓ Workspaces collected: {len(workspaces_info)}\\n\")\n","\n","# ==============================================================  \n","# EXTRACT ENVIRONMENT METADATA\n","# ==============================================================\n","\n","# Create a single REST client instance to reuse\n","client = FabricRestClient()\n","\n","for ws_info in workspaces_info:\n","    ws_name = ws_info[\"WorkspaceName\"]\n","    ws_id = ws_info[\"WorkspaceId\"]\n","    \n","    log(f\"\\nProcessing workspace: {ws_name} | Elapsed: {elapsed_min():.2f} min\")\n","\n","    # -------------------- DATASETS --------------------\n","    try:\n","        log(f\"  Fetching datasets...\")\n","        datasets_df = fabric.list_datasets(workspace=ws_name)\n","        \n","        if datasets_df is not None and not datasets_df.empty:\n","            log(f\"  Datasets found: {len(datasets_df)}\")\n","            \n","            for _, ds_row in datasets_df.iterrows():\n","                dataset_id = safe_get(ds_row, \"Dataset ID\")\n","                dataset_name = safe_get(ds_row, \"Dataset Name\")\n","                \n","                # Store in lookup\n","                dataset_name_lookup[dataset_id] = dataset_name\n","                \n","                datasets_info.append({\n","                    \"WorkspaceId\": ws_id,\n","                    \"WorkspaceName\": ws_name,\n","                    \"DatasetId\": dataset_id,\n","                    \"DatasetName\": dataset_name,\n","                    \"DatasetDescription\": safe_get(ds_row, \"Description\"),\n","                    \"DatasetWebUrl\": safe_get(ds_row, \"Web URL\"),\n","                    \"DatasetConfiguredBy\": safe_get(ds_row, \"Configured By\"),\n","                    \"DatasetIsRefreshable\": safe_get(ds_row, \"Is Refreshable\", False),\n","                    \"DatasetTargetStorageMode\": safe_get(ds_row, \"Target Storage Mode\"),\n","                    \"DatasetCreatedDate\": safe_get(ds_row, \"Created Date\")\n","                })\n","                \n","                # Fetch dataset sources using REST API\n","                try:\n","                    datasources_url = f\"v1.0/myorg/groups/{ws_id}/datasets/{dataset_id}/datasources\"\n","                    response = client.get(datasources_url)\n","                    \n","                    if response.status_code == 200:\n","                        datasources = response.json().get('value', [])\n","                        for datasource in datasources:\n","                            dataset_sources_info.append({\n","                                \"WorkspaceId\": ws_id,\n","                                \"WorkspaceName\": ws_name,\n","                                \"DatasetId\": dataset_id,\n","                                \"DatasetName\": dataset_name,\n","                                \"DatasetDatasourceType\": datasource.get(\"datasourceType\", \"\"),\n","                                \"DatasetDatasourceId\": datasource.get(\"datasourceId\", \"\"),\n","                                \"DatasetDatasourceGatewayId\": datasource.get(\"gatewayId\", \"\"),\n","                                \"DatasetDatasourceConnectionDetails\": serialize_json(datasource.get(\"connectionDetails\"))\n","                            })\n","                except Exception as e:\n","                    log(f\"    Could not fetch dataset sources for {dataset_name}: {e}\")\n","                \n","                # Fetch dataset refresh history\n","                try:\n","                    refresh_url = f\"v1.0/myorg/groups/{ws_id}/datasets/{dataset_id}/refreshes\"\n","                    response = client.get(refresh_url)\n","                    \n","                    if response.status_code == 200:\n","                        refreshes = response.json().get('value', [])\n","                        for refresh in refreshes:\n","                            dataset_refresh_history.append({\n","                                \"WorkspaceId\": ws_id,\n","                                \"WorkspaceName\": ws_name,\n","                                \"DatasetId\": dataset_id,\n","                                \"DatasetName\": dataset_name,\n","                                \"DatasetRefreshRequestId\": refresh.get(\"requestId\", \"\"),\n","                                \"DatasetRefreshId\": refresh.get(\"id\", \"\"),\n","                                \"DatasetRefreshStartTime\": refresh.get(\"startTime\", \"\"),\n","                                \"DatasetRefreshEndTime\": refresh.get(\"endTime\", \"\"),\n","                                \"DatasetRefreshStatus\": refresh.get(\"status\", \"\"),\n","                                \"DatasetRefreshType\": refresh.get(\"refreshType\", \"\")\n","                            })\n","                except Exception as e:\n","                    log(f\"    Could not fetch refresh history for {dataset_name}: {e}\")\n","        else:\n","            log(f\"  No datasets found\")\n","            \n","    except Exception as e:\n","        log(f\"  ERROR fetching datasets: {e}\")\n","\n","    # -------------------- DATAFLOWS --------------------\n","    try:\n","        log(f\"  Fetching dataflows...\")\n","        dataflows_url = f\"v1.0/myorg/groups/{ws_id}/dataflows\"\n","        response = client.get(dataflows_url)\n","        \n","        if response.status_code == 200:\n","            dataflows = response.json().get('value', [])\n","            log(f\"  Dataflows found: {len(dataflows)}\")\n","            \n","            for dataflow in dataflows:\n","                dataflow_id = dataflow.get(\"objectId\", \"\")\n","                dataflow_name = dataflow.get(\"name\", \"\")\n","                \n","                # Store in lookup\n","                if dataflow_id:\n","                    dataflow_name_lookup[dataflow_id] = dataflow_name\n","                \n","                dataflows_info.append({\n","                    \"WorkspaceId\": ws_id,\n","                    \"WorkspaceName\": ws_name,\n","                    \"DataflowId\": dataflow_id,\n","                    \"DataflowName\": dataflow_name,\n","                    \"DataflowDescription\": dataflow.get(\"description\", \"\"),\n","                    \"DataflowConfiguredBy\": dataflow.get(\"configuredBy\", \"\"),\n","                    \"DataflowModifiedBy\": dataflow.get(\"modifiedBy\", \"\"),\n","                    \"DataflowModifiedDateTime\": dataflow.get(\"modifiedDateTime\", \"\"),\n","                    \"DataflowJsonURL\": dataflow.get(\"modelUrl\", \"\"),\n","                    \"DataflowGeneration\": dataflow.get(\"generation\", \"\")\n","                })\n","                \n","                # Fetch dataflow sources\n","                try:\n","                    dataflow_sources_url = f\"v1.0/myorg/groups/{ws_id}/dataflows/{dataflow_id}/datasources\"\n","                    sources_response = client.get(dataflow_sources_url)\n","                    \n","                    if sources_response.status_code == 200:\n","                        sources = sources_response.json().get('value', [])\n","                        for source in sources:\n","                            dataflow_sources_info.append({\n","                                \"WorkspaceId\": ws_id,\n","                                \"WorkspaceName\": ws_name,\n","                                \"DataflowId\": dataflow_id,\n","                                \"DataflowName\": dataflow_name,\n","                                \"DataflowDatasourceType\": source.get(\"datasourceType\", \"\"),\n","                                \"DataflowDatasourceId\": source.get(\"datasourceId\", \"\"),\n","                                \"DataflowDatasourceGatewayId\": source.get(\"gatewayId\", \"\"),\n","                                \"DataflowDatasourceConnectionDetails\": serialize_json(source.get(\"connectionDetails\"))\n","                            })\n","                except Exception as e:\n","                    log(f\"    Could not fetch dataflow sources for {dataflow_name}: {e}\")\n","                \n","                # Fetch dataflow refresh history (transactions)\n","                try:\n","                    refresh_url = f\"v1.0/myorg/groups/{ws_id}/dataflows/{dataflow_id}/transactions\"\n","                    refresh_response = client.get(refresh_url)\n","                    \n","                    if refresh_response.status_code == 200:\n","                        refreshes = refresh_response.json().get('value', [])\n","                        for refresh in refreshes:\n","                            dataflow_refresh_history.append({\n","                                \"WorkspaceId\": ws_id,\n","                                \"WorkspaceName\": ws_name,\n","                                \"DataflowId\": dataflow_id,\n","                                \"DataflowName\": dataflow_name,\n","                                \"DataflowRefreshRequestId\": refresh.get(\"requestId\", \"\"),\n","                                \"DataflowRefreshId\": refresh.get(\"id\", \"\"),\n","                                \"DataflowRefreshStartTime\": refresh.get(\"startTime\", \"\"),\n","                                \"DataflowRefreshEndTime\": refresh.get(\"endTime\", \"\"),\n","                                \"DataflowRefreshStatus\": refresh.get(\"status\", \"\"),\n","                                \"DataflowRefreshType\": refresh.get(\"refreshType\", \"\"),\n","                                \"DataflowErrorInfo\": serialize_json(refresh.get(\"errorInfo\"))\n","                            })\n","                except Exception as e:\n","                    log(f\"    Could not fetch refresh history for {dataflow_name}: {e}\")\n","        else:\n","            log(f\"  No dataflows found\")\n","    except Exception as e:\n","        log(f\"  ERROR fetching dataflows: {e}\")\n","\n","    # -------------------- FABRIC ITEMS --------------------\n","    try:\n","        log(f\"  Fetching Fabric items...\")\n","        items_url = f\"v1.0/workspaces/{ws_id}/items\"\n","        response = client.get(items_url)\n","        \n","        if response.status_code == 200:\n","            items = response.json().get('value', [])\n","            # Filter out Reports and SemanticModels as they're handled separately\n","            filtered_items = [item for item in items if item.get('type') not in ['Report', 'SemanticModel']]\n","            \n","            log(f\"  Fabric items found: {len(filtered_items)}\")\n","            \n","            for item in filtered_items:\n","                fabric_items_info.append({\n","                    \"WorkspaceId\": ws_id,\n","                    \"WorkspaceName\": ws_name,\n","                    \"FabricItemID\": item.get(\"id\", \"\"),\n","                    \"FabricItemType\": item.get(\"type\", \"\"),\n","                    \"FabricItemName\": item.get(\"displayName\", \"\"),\n","                    \"FabricItemDescription\": item.get(\"description\", \"\")\n","                })\n","        else:\n","            log(f\"  No Fabric items found\")\n","    except Exception as e:\n","        log(f\"  ERROR fetching Fabric items: {e}\")\n","\n","    # -------------------- REPORTS --------------------\n","    try:\n","        log(f\"  Fetching reports...\")\n","        reports_df = fabric.list_reports(workspace=ws_name)\n","        \n","        if reports_df is not None and not reports_df.empty:\n","            log(f\"  Reports found: {len(reports_df)}\")\n","            \n","            for _, rpt_row in reports_df.iterrows():\n","                report_id = safe_get(rpt_row, \"Id\")\n","                report_name = safe_get(rpt_row, \"Name\")\n","                dataset_id = safe_get(rpt_row, \"Dataset Id\")\n","                \n","                # Get dataset name from lookup\n","                dataset_name = dataset_name_lookup.get(dataset_id, \"Unknown Dataset\")\n","                \n","                reports_info.append({\n","                    \"WorkspaceId\": ws_id,\n","                    \"WorkspaceName\": ws_name,\n","                    \"ReportId\": report_id,\n","                    \"ReportName\": report_name,\n","                    \"ReportDescription\": safe_get(rpt_row, \"Description\"),\n","                    \"ReportWebUrl\": safe_get(rpt_row, \"Web URL\"),\n","                    \"ReportEmbedUrl\": safe_get(rpt_row, \"Embed URL\"),\n","                    \"ReportType\": safe_get(rpt_row, \"Report Type\"),\n","                    \"DatasetId\": dataset_id,\n","                    \"DatasetName\": dataset_name\n","                })\n","                \n","                # Fetch report pages using REST API\n","                try:\n","                    pages_url = f\"v1.0/myorg/groups/{ws_id}/reports/{report_id}/pages\"\n","                    pages_response = client.get(pages_url)\n","                    \n","                    if pages_response.status_code == 200:\n","                        pages = pages_response.json().get('value', [])\n","                        for page in pages:\n","                            report_pages_info.append({\n","                                \"WorkspaceId\": ws_id,\n","                                \"WorkspaceName\": ws_name,\n","                                \"ReportId\": report_id,\n","                                \"ReportName\": report_name,\n","                                \"PageName\": page.get(\"name\", \"\"),\n","                                \"PageDisplayName\": page.get(\"displayName\", \"\"),\n","                                \"PageOrder\": page.get(\"order\", 0)\n","                            })\n","                except Exception as e:\n","                    log(f\"    ERROR fetching pages for {report_name}: {e}\")\n","        else:\n","            log(f\"  No reports found\")\n","            \n","    except Exception as e:\n","        log(f\"  ERROR fetching reports: {e}\")\n","\n","    log(f\"✓ Finished workspace: {ws_name}\")\n","\n","# ==============================================================  \n","# APPS AND APP REPORTS\n","# ==============================================================\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"Fetching Apps and App Reports\")\n","log(\"=\"*80)\n","\n","try:\n","    apps_url = \"v1.0/myorg/apps\"\n","    response = client.get(apps_url)\n","    \n","    if response.status_code == 200:\n","        apps = response.json().get('value', [])\n","        log(f\"Apps found: {len(apps)}\")\n","        \n","        # Filter to only apps in our workspaces (create list once)\n","        workspace_ids = [ws['WorkspaceId'] for ws in workspaces_info]\n","        # Create workspace ID to name lookup\n","        workspace_name_lookup = {ws['WorkspaceId']: ws['WorkspaceName'] for ws in workspaces_info}\n","        \n","        for app in apps:\n","            app_workspace_id = app.get(\"workspaceId\", \"\")\n","            \n","            if app_workspace_id in workspace_ids:\n","                app_id = app.get(\"id\", \"\")\n","                app_name = app.get(\"name\", \"\")\n","                app_workspace_name = workspace_name_lookup.get(app_workspace_id, \"\")\n","                \n","                apps_info.append({\n","                    \"AppId\": app_id,\n","                    \"AppName\": app_name,\n","                    \"AppLastUpdate\": app.get(\"lastUpdate\", \"\"),\n","                    \"AppDescription\": app.get(\"description\", \"\"),\n","                    \"AppPublishedBy\": app.get(\"publishedBy\", \"\"),\n","                    \"AppWorkspaceId\": app_workspace_id,\n","                    \"WorkspaceName\": app_workspace_name\n","                })\n","                \n","                # Fetch reports within each app\n","                try:\n","                    app_reports_url = f\"v1.0/myorg/apps/{app_id}/reports\"\n","                    app_reports_response = client.get(app_reports_url)\n","                    \n","                    if app_reports_response.status_code == 200:\n","                        app_reports = app_reports_response.json().get('value', [])\n","                        \n","                        for report in app_reports:\n","                            reports_in_app_info.append({\n","                                \"AppId\": app_id,\n","                                \"AppName\": app_name,\n","                                \"AppReportId\": report.get(\"id\", \"\"),\n","                                \"AppReportType\": report.get(\"reportType\", \"\"),\n","                                \"ReportName\": report.get(\"name\", \"\"),\n","                                \"AppReportWebUrl\": report.get(\"webUrl\", \"\"),\n","                                \"AppReportEmbedUrl\": report.get(\"embedUrl\", \"\"),\n","                                \"AppReportIsOwnedByMe\": report.get(\"isOwnedByMe\", False),\n","                                \"AppReportDatasetId\": report.get(\"datasetId\", \"\"),\n","                                \"ReportId\": report.get(\"originalReportObjectId\", \"\"),\n","                                \"WorkspaceName\": app_workspace_name\n","                            })\n","                except Exception as e:\n","                    log(f\"  ERROR fetching app reports for {app_name}: {e}\")\n","                    \n","    else:\n","        log(f\"No apps found or unable to fetch apps\")\n","        \n","except Exception as e:\n","    log(f\"ERROR fetching apps: {e}\")\n","\n","# ==============================================================  \n","# DATAFLOW LINEAGE\n","# ==============================================================\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"Fetching Dataflow Lineage\")\n","log(\"=\"*80)\n","\n","for ws_info in workspaces_info:\n","    ws_name = ws_info[\"WorkspaceName\"]\n","    ws_id = ws_info[\"WorkspaceId\"]\n","    \n","    try:\n","        lineage_url = f\"v1.0/myorg/groups/{ws_id}/dataflows/upstreamDataflows\"\n","        response = client.get(lineage_url)\n","        \n","        if response.status_code == 200:\n","            lineage_items = response.json().get('value', [])\n","            \n","            for lineage in lineage_items:\n","                dataflow_id = lineage.get(\"dataflowObjectId\", \"\")\n","                dataset_id = lineage.get(\"datasetObjectId\", \"\")\n","                \n","                dataflow_lineage.append({\n","                    \"WorkspaceId\": ws_id,\n","                    \"WorkspaceName\": ws_name,\n","                    \"DataflowId\": dataflow_id,\n","                    \"DataflowName\": dataflow_name_lookup.get(dataflow_id, \"Unknown Dataflow\"),\n","                    \"DatasetId\": dataset_id,\n","                    \"DatasetName\": dataset_name_lookup.get(dataset_id, \"Unknown Dataset\")\n","                })\n","    except Exception as e:\n","        log(f\"  Could not fetch dataflow lineage for {ws_name}: {e}\")\n","\n","log(\"✓ Dataflow lineage collection complete\")\n","\n","# ==============================================================  \n","# WRITE TO LAKEHOUSE\n","# ==============================================================\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"Writing output to Lakehouse\")\n","log(\"=\"*80)\n","\n","def write_table(data, name, sample_row=None):\n","    full_name = f\"{CATALOG}.{LAKEHOUSE_NAME}.{name}\"\n","    \n","    if not data:\n","        # Create empty table using sample row structure if provided\n","        if sample_row:\n","            log(f\"Creating empty table with schema: {name}\")\n","            pandas_df = pd.DataFrame([sample_row])\n","            df = spark.createDataFrame(pandas_df)\n","            # Filter to create empty dataframe with schema\n","            empty_df = df.filter(\"1=0\")\n","            empty_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","            log(f\"✓ Created empty table: {full_name}\\n\")\n","        else:\n","            log(f\"⚠ Empty table skipped (no schema): {name}\\n\")\n","        return\n","\n","    # Convert to pandas DataFrame first for proper type handling, then to Spark\n","    pandas_df = pd.DataFrame(data)\n","    df = spark.createDataFrame(pandas_df)\n","    count = df.count()\n","\n","    log(f\"Writing {count} rows → {full_name}\")\n","\n","    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","\n","    log(f\"✓ Wrote table: {full_name}\\n\")\n","\n","# Write all tables matching PowerShell script worksheets\n","write_table(workspaces_info, \"Workspaces\", SAMPLE_ROWS.get(\"Workspaces\"))\n","write_table(fabric_items_info, \"FabricItems\", SAMPLE_ROWS.get(\"FabricItems\"))\n","write_table(datasets_info, \"Datasets\", SAMPLE_ROWS.get(\"Datasets\"))\n","write_table(dataset_sources_info, \"DatasetSourcesInfo\", SAMPLE_ROWS.get(\"DatasetSourcesInfo\"))\n","write_table(dataset_refresh_history, \"DatasetRefreshHistory\", SAMPLE_ROWS.get(\"DatasetRefreshHistory\"))\n","write_table(dataflows_info, \"Dataflows\", SAMPLE_ROWS.get(\"Dataflows\"))\n","write_table(dataflow_lineage, \"DataflowLineage\", SAMPLE_ROWS.get(\"DataflowLineage\"))\n","write_table(dataflow_sources_info, \"DataflowSourcesInfo\", SAMPLE_ROWS.get(\"DataflowSourcesInfo\"))\n","write_table(dataflow_refresh_history, \"DataflowRefreshHistory\", SAMPLE_ROWS.get(\"DataflowRefreshHistory\"))\n","write_table(reports_info, \"Reports\", SAMPLE_ROWS.get(\"Reports\"))\n","write_table(report_pages_info, \"ReportPages\", SAMPLE_ROWS.get(\"ReportPages\"))\n","write_table(apps_info, \"Apps\", SAMPLE_ROWS.get(\"Apps\"))\n","write_table(reports_in_app_info, \"AppReports\", SAMPLE_ROWS.get(\"AppReports\"))\n","\n","# ==============================================================  \n","# END\n","# ==============================================================\n","\n","heartbeat_running = False\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"PROCESS COMPLETE\")\n","log(f\"Finished at: {datetime.now()}\")\n","log(f\"Total runtime: {elapsed_min():.2f} minutes\")\n","log(\"=\"*80)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"651c670b-fbf0-4e9e-ad2b-f3e2d1c5a938"},{"cell_type":"code","source":["# ================================\n","# FABRIC REPORT METADATA EXTRACTOR (ReportWrapper Only)\n","# WITH AUTO-SCHEMA CREATION\n","# ================================\n","\n","# %pip install semantic-link-labs --quiet\n","\n","import time, re, pandas as pd\n","from datetime import datetime\n","import sempy.fabric as fabric\n","from sempy_labs.report import ReportWrapper\n","# Note: Using private module for resolve_dataset_from_report - consider this dependency if upgrading semantic-link-labs\n","from sempy_labs._helper_functions import resolve_dataset_from_report\n","\n","# -----------------------------------\n","# CONFIG\n","# -----------------------------------\n","LAKEHOUSE_NAME = \"dbo\"         # <-- CHANGE THIS\n","SINGLE_WORKSPACE_NAME = None   # <-- or set to None to scan all\n","\n","# Validate lakehouse name\n","if not re.match(r'^[a-zA-Z0-9_]+$', LAKEHOUSE_NAME):\n","    raise ValueError(f\"Invalid lakehouse name: {LAKEHOUSE_NAME}\")\n","\n","EXTRACTION_TIMESTAMP = datetime.now()\n","REPORT_DATE = EXTRACTION_TIMESTAMP.strftime(\"%Y-%m-%d\")\n","start_time = time.time()\n","\n","# -----------------------------------\n","# Logging helpers\n","# -----------------------------------\n","def log(msg):\n","    print(msg, flush=True)\n","\n","def elapsed_min():\n","    return (time.time() - start_time) / 60\n","\n","# Heartbeat\n","import threading\n","heartbeat_running = True\n","def heartbeat():\n","    while heartbeat_running:\n","        time.sleep(10)\n","        print(f\"[Heartbeat] Still running… elapsed {elapsed_min():.2f} min\", flush=True)\n","\n","threading.Thread(target=heartbeat, daemon=True).start()\n","\n","# -----------------------------------\n","# Start banner\n","# -----------------------------------\n","log(\"=\"*80)\n","log(\"FABRIC REPORT METADATA EXTRACTION\")\n","log(f\"Started: {EXTRACTION_TIMESTAMP}\")\n","log(\"=\"*80)\n","\n","# ============================================\n","# AUTO-CREATE SCHEMA (LAKEHOUSE)\n","# ============================================\n","CATALOG = spark.sql(\"SELECT current_catalog()\").first()[0]\n","log(f\"Using catalog: {CATALOG}\")\n","\n","schema_name = f\"{CATALOG}.{LAKEHOUSE_NAME}\"\n","log(f\"Ensuring lakehouse schema exists: {schema_name}\")\n","\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n","log(f\"✓ Schema is ready: {schema_name}\\n\")\n","\n","\n","\n","# ==============================================================  \n","# COLLECTIONS & SCHEMA TEMPLATES\n","# ==============================================================\n","# Each collection includes a template row that defines the schema.\n","# This ensures empty tables can be created with correct column structure.\n","\n","all_connections = [{\"ReportID\": \"\", \"ModelID\": \"\", \"ReportDate\": \"\", \"ReportName\": \"\", \"Type\": \"\", \"ServerName\": \"\", \"WorkspaceName\": \"\"}]\n","all_pages = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"Id\": \"\", \"Name\": \"\", \"Number\": 0, \"Width\": 0, \"Height\": 0, \"HiddenFlag\": False, \"VisualCount\": 0, \"Type\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_visuals = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"PageName\": \"\", \"PageId\": \"\", \"Id\": \"\", \"Name\": \"\", \"Type\": \"\", \"CustomVisualFlag\": False, \"HiddenFlag\": False, \"X\": 0.0, \"Y\": 0.0, \"Z\": 0, \"Width\": 0.0, \"Height\": 0.0, \"ObjectCount\": 0, \"ParentGroup\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_bookmarks = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"Name\": \"\", \"Id\": \"\", \"PageName\": \"\", \"PageId\": \"\", \"VisualId\": \"\", \"VisualHiddenFlag\": False, \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_custom_visuals = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"Name\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_report_filters = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"displayName\": \"\", \"TableName\": \"\", \"ObjectName\": \"\", \"ObjectType\": \"\", \"FilterType\": \"\", \"HiddenFilter\": \"\", \"LockedFilter\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_page_filters = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"PageId\": \"\", \"PageName\": \"\", \"displayName\": \"\", \"TableName\": \"\", \"ObjectName\": \"\", \"ObjectType\": \"\", \"FilterType\": \"\", \"HiddenFilter\": \"\", \"LockedFilter\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_visual_filters = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"PageName\": \"\", \"PageId\": \"\", \"VisualId\": \"\", \"TableName\": \"\", \"ObjectName\": \"\", \"ObjectType\": \"\", \"FilterType\": \"\", \"HiddenFilter\": \"\", \"LockedFilter\": \"\", \"displayName\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_visual_objects = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"PageName\": \"\", \"PageId\": \"\", \"VisualId\": \"\", \"VisualType\": \"\", \"CustomVisualFlag\": False, \"TableName\": \"\", \"ObjectName\": \"\", \"ObjectType\": \"\", \"Source\": \"\", \"displayName\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_report_level_measures = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"TableName\": \"\", \"ObjectName\": \"\", \"ObjectType\": \"\", \"Expression\": \"\", \"HiddenFlag\": \"\", \"FormatString\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","all_visual_interactions = [{\"ReportName\": \"\", \"ReportID\": \"\", \"ModelID\": \"\", \"PageName\": \"\", \"PageId\": \"\", \"SourceVisualID\": \"\", \"TargetVisualID\": \"\", \"TypeID\": \"\", \"Type\": \"\", \"ReportDate\": \"\", \"WorkspaceName\": \"\"}]\n","\n","# ==============================================================  \n","# GET WORKSPACES\n","# ==============================================================\n","\n","workspaces_df = fabric.list_workspaces()\n","\n","if SINGLE_WORKSPACE_NAME:\n","    workspaces_df = workspaces_df[workspaces_df[\"Name\"] == SINGLE_WORKSPACE_NAME]\n","    if workspaces_df.empty:\n","        raise ValueError(f\"Workspace '{SINGLE_WORKSPACE_NAME}' not found.\")\n","    log(f\"Filtering to workspace: {SINGLE_WORKSPACE_NAME}\")\n","\n","log(f\"Workspace count: {len(workspaces_df)}\")\n","log(\"\")\n","\n","# ==============================================================  \n","# REPORT METADATA EXTRACTION\n","# ==============================================================\n","\n","for ws_row in workspaces_df.itertuples(index=False):\n","    ws_name = ws_row.Name\n","    log(f\"\\nProcessing workspace: {ws_name} | Elapsed: {elapsed_min():.2f} min\")\n","\n","    try:\n","        reports_df = fabric.list_reports(workspace=ws_name)\n","        if reports_df is None or reports_df.empty:\n","            log(\"  No reports found.\")\n","            continue\n","\n","        log(f\"  Reports found: {len(reports_df)}\")\n","\n","        for idx, rpt_row in enumerate(reports_df.itertuples(index=False), start=1):\n","            rpt_name = rpt_row.Name\n","            rpt_id = rpt_row.Id\n","            \n","            # Get dataset/model ID - try from list_reports first, then use API as fallback\n","            model_id = \"\"\n","            if hasattr(rpt_row, 'DatasetId') and rpt_row.DatasetId is not None:\n","                model_id = str(rpt_row.DatasetId)\n","            \n","            if not model_id:\n","                try:\n","                    # resolve_dataset_from_report returns: (dataset_id, dataset_name, workspace_id, workspace_name)\n","                    dataset_id, _, _, _ = resolve_dataset_from_report(\n","                        report=rpt_id, workspace=ws_name\n","                    )\n","                    model_id = str(dataset_id) if dataset_id is not None else \"\"\n","                except Exception as e:\n","                    log(f\"    Warning: Could not resolve dataset ID: {e}\")\n","                    model_id = \"\"\n","\n","            # -------------------- Connections --------------------\n","            # Add connection record (one per report)\n","            all_connections.append({\n","                \"ReportID\": rpt_id,\n","                \"ModelID\": model_id,\n","                \"ReportDate\": REPORT_DATE,\n","                \"ReportName\": rpt_name,\n","                \"Type\": \"\",\n","                \"ServerName\": \"\",\n","                \"WorkspaceName\": ws_name\n","            })\n","\n","            t0 = time.time()\n","            log(f\"\\n  [{idx}/{len(reports_df)}] Extracting report: {rpt_name}\")\n","\n","            try:\n","                rpt = ReportWrapper(report=rpt_name, workspace=ws_name)\n","\n","                # -------------------- Pages --------------------\n","                df = rpt.list_pages()\n","                log(f\"    Pages: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_pages.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"Id\": row.get(\"Page Name\", \"\"),\n","                            \"Name\": row.get(\"Page Display Name\", \"\"),\n","                            \"Number\": 0,  # Note: Page number not available in list_pages() output\n","                            \"Width\": row.get(\"Width\", 0),\n","                            \"Height\": row.get(\"Height\", 0),\n","                            \"HiddenFlag\": bool(row.get(\"Hidden\", False)),\n","                            \"VisualCount\": row.get(\"Visual Count\", 0),\n","                            \"Type\": row.get(\"Display Option\", \"\"),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Visuals --------------------\n","                df = rpt.list_visuals()\n","                log(f\"    Visuals: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_visuals.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"PageName\": row.get(\"Page Display Name\", \"\"),\n","                            \"PageId\": row.get(\"Page Name\", \"\"),\n","                            \"Id\": row.get(\"Visual Name\", \"\"),\n","                            \"Name\": row.get(\"Visual Name\", \"\"),\n","                            \"Type\": row.get(\"Type\", \"\"),\n","                            \"CustomVisualFlag\": bool(row.get(\"Custom Visual\", False)),\n","                            \"HiddenFlag\": bool(row.get(\"Hidden\", False)),\n","                            \"X\": row.get(\"X\", 0),\n","                            \"Y\": row.get(\"Y\", 0),\n","                            \"Z\": row.get(\"Z\", 0),\n","                            \"Width\": row.get(\"Width\", 0),\n","                            \"Height\": row.get(\"Height\", 0),\n","                            \"ObjectCount\": row.get(\"Visual Object Count\", 0),\n","                            \"ParentGroup\": \"\",  # Note: Parent group not available in list_visuals() output\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Bookmarks --------------------\n","                df = rpt.list_bookmarks()\n","                log(f\"    Bookmarks: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_bookmarks.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"Name\": row.get(\"Bookmark Display Name\", \"\"),\n","                            \"Id\": row.get(\"Bookmark Name\", \"\"),\n","                            \"PageName\": row.get(\"Page Display Name\", \"\"),\n","                            \"PageId\": row.get(\"Page Name\", \"\"),\n","                            \"VisualId\": row.get(\"Visual Name\", \"\"),\n","                            \"VisualHiddenFlag\": bool(row.get(\"Visual Hidden\", False)),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Custom Visuals --------------------\n","                df = rpt.list_custom_visuals()\n","                log(f\"    Custom Visuals: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_custom_visuals.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"Name\": row.get(\"Custom Visual Display Name\", \"\"),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Report Filters --------------------\n","                df = rpt.list_report_filters()\n","                log(f\"    Report Filters: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_report_filters.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"displayName\": row.get(\"Filter Name\", \"\"),\n","                            \"TableName\": row.get(\"Table Name\", \"\"),\n","                            \"ObjectName\": row.get(\"Object Name\", \"\"),\n","                            \"ObjectType\": row.get(\"Object Type\", \"\"),\n","                            \"FilterType\": row.get(\"Type\", \"\"),\n","                            \"HiddenFilter\": str(bool(row.get(\"Hidden\", False))),\n","                            \"LockedFilter\": str(bool(row.get(\"Locked\", False))),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Page Filters --------------------\n","                df = rpt.list_page_filters()\n","                log(f\"    Page Filters: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_page_filters.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"PageId\": row.get(\"Page Name\", \"\"),\n","                            \"PageName\": row.get(\"Page Display Name\", \"\"),\n","                            \"displayName\": row.get(\"Filter Name\", \"\"),\n","                            \"TableName\": row.get(\"Table Name\", \"\"),\n","                            \"ObjectName\": row.get(\"Object Name\", \"\"),\n","                            \"ObjectType\": row.get(\"Object Type\", \"\"),\n","                            \"FilterType\": row.get(\"Type\", \"\"),\n","                            \"HiddenFilter\": str(bool(row.get(\"Hidden\", False))),\n","                            \"LockedFilter\": str(bool(row.get(\"Locked\", False))),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Visual Filters --------------------\n","                df = rpt.list_visual_filters()\n","                log(f\"    Visual Filters: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_visual_filters.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"PageName\": row.get(\"Page Display Name\", \"\"),\n","                            \"PageId\": row.get(\"Page Name\", \"\"),\n","                            \"VisualId\": row.get(\"Visual Name\", \"\"),\n","                            \"TableName\": row.get(\"Table Name\", \"\"),\n","                            \"ObjectName\": row.get(\"Object Name\", \"\"),\n","                            \"ObjectType\": row.get(\"Object Type\", \"\"),\n","                            \"FilterType\": row.get(\"Type\", \"\"),\n","                            \"HiddenFilter\": str(bool(row.get(\"Hidden\", False))),\n","                            \"LockedFilter\": str(bool(row.get(\"Locked\", False))),\n","                            \"displayName\": row.get(\"Filter Name\", \"\"),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Visual Objects --------------------\n","                df = rpt.list_visual_objects()\n","                log(f\"    Visual Objects: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_visual_objects.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"PageName\": row.get(\"Page Display Name\", \"\"),\n","                            \"PageId\": row.get(\"Page Name\", \"\"),\n","                            \"VisualId\": row.get(\"Visual Name\", \"\"),\n","                            \"VisualType\": \"\",  # Note: Visual type not available in list_visual_objects() - join with Visuals table if needed\n","                            \"CustomVisualFlag\": False,  # Note: Custom visual flag not available in list_visual_objects() - join with Visuals table if needed\n","                            \"TableName\": row.get(\"Table Name\", \"\"),\n","                            \"ObjectName\": row.get(\"Object Name\", \"\"),\n","                            \"ObjectType\": row.get(\"Object Type\", \"\"),\n","                            \"Source\": \"\",  # Note: Source not available in list_visual_objects() output\n","                            \"displayName\": row.get(\"Object Display Name\", \"\"),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Report-Level Measures --------------------\n","                df = rpt.list_report_level_measures()\n","                log(f\"    Report-Level Measures: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_report_level_measures.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"TableName\": row.get(\"Table Name\", \"\"),\n","                            \"ObjectName\": row.get(\"Measure Name\", \"\"),\n","                            \"ObjectType\": \"Measure\",\n","                            \"Expression\": row.get(\"Expression\", \"\"),\n","                            \"HiddenFlag\": \"False\",  # Note: Hidden flag not available in list_report_level_measures() - report-level measures are typically visible\n","                            \"FormatString\": row.get(\"Format String\", \"\"),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","                # -------------------- Visual Interactions --------------------\n","                df = rpt.list_visual_interactions()\n","                log(f\"    Visual Interactions: {0 if df is None else len(df)}\")\n","                if isinstance(df, pd.DataFrame) and not df.empty:\n","                    for _, row in df.iterrows():\n","                        all_visual_interactions.append({\n","                            \"ReportName\": rpt_name,\n","                            \"ReportID\": rpt_id,\n","                            \"ModelID\": model_id,\n","                            \"PageName\": row.get(\"Page Display Name\", \"\"),\n","                            \"PageId\": row.get(\"Page Name\", \"\"),\n","                            \"SourceVisualID\": row.get(\"Source Visual Name\", \"\"),\n","                            \"TargetVisualID\": row.get(\"Target Visual Name\", \"\"),\n","                            \"TypeID\": \"\",  # Note: TypeID not available from semantic-link-labs\n","                            \"Type\": row.get(\"Type\", \"\"),\n","                            \"ReportDate\": REPORT_DATE,\n","                            \"WorkspaceName\": ws_name\n","                        })\n","\n","            except Exception as e:\n","                log(f\"    ERROR extracting {rpt_name}: {e}\")\n","\n","            log(f\"  → Finished {rpt_name} in {time.time() - t0:.1f} sec \"\n","                f\"(Total: {elapsed_min():.2f} min)\")\n","\n","    except Exception as e:\n","        log(f\"ERROR accessing workspace {ws_name}: {e}\")\n","\n","# ==============================================================  \n","# WRITE TO LAKEHOUSE\n","# ==============================================================\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"Writing output to Lakehouse\")\n","log(\"=\"*80)\n","\n","def write_table(data, name):\n","    \"\"\"\n","    Write data to a Delta table. Schema is inferred from the first row (template).\n","    Creates empty table with schema if only template row exists.\n","    \n","    Args:\n","        data: List of dictionaries containing the data (first row is schema template)\n","        name: Name of the table\n","    \"\"\"\n","    full_name = f\"{CATALOG}.{LAKEHOUSE_NAME}.{name}\"\n","    \n","    # Check if we only have the template row (length 1 means just the schema template)\n","    if len(data) == 1:\n","        log(f\"⚠ No data for {name}, creating empty table with schema\")\n","        # Use template to create empty DataFrame with correct schema\n","        df = spark.createDataFrame(pd.DataFrame(data))\n","        # Filter out the template row to create truly empty table\n","        empty_df = df.filter(\"1=0\")\n","        empty_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","        log(f\"✓ Created empty table: {full_name}\\n\")\n","        return\n","\n","    # Skip the template row (first row) and create DataFrame with actual data\n","    pandas_df = pd.DataFrame(data)\n","    actual_df = spark.createDataFrame(pandas_df.iloc[1:])\n","    count = actual_df.count()\n","\n","    log(f\"Writing {count} rows → {full_name}\")\n","\n","    actual_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","\n","    log(f\"✓ Wrote table: {full_name}\\n\")\n","\n","write_table(all_connections, \"Connections\")\n","write_table(all_pages, \"Pages\")\n","write_table(all_visuals, \"Visuals\")\n","write_table(all_bookmarks, \"Bookmarks\")\n","write_table(all_custom_visuals, \"CustomVisuals\")\n","write_table(all_report_filters, \"ReportFilters\")\n","write_table(all_page_filters, \"PageFilters\")\n","write_table(all_visual_filters, \"VisualFilters\")\n","write_table(all_visual_objects, \"VisualObjects\")\n","write_table(all_report_level_measures, \"ReportLevelMeasures\")\n","write_table(all_visual_interactions, \"VisualInteractions\")\n","\n","# ==============================================================  \n","# END\n","# ==============================================================\n","\n","heartbeat_running = False\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"PROCESS COMPLETE\")\n","log(f\"Finished at: {datetime.now()}\")\n","log(f\"Total runtime: {elapsed_min():.2f} minutes\")\n","log(\"=\"*80)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcf7594b-2309-4d3c-8e78-9422a3f9c0e2"},{"cell_type":"code","source":["# ================================\n","# FABRIC MODEL METADATA EXTRACTOR (TOMWrapper)\n","# WITH AUTO-SCHEMA CREATION\n","# ================================\n","\n","# %pip install semantic-link-labs --quiet\n","\n","import time, re, pandas as pd\n","from datetime import datetime\n","import sempy.fabric as fabric\n","from sempy_labs.tom import TOMWrapper\n","from sempy_labs._model_dependencies import get_model_calc_dependencies\n","\n","# -----------------------------------\n","# CONFIG\n","# -----------------------------------\n","LAKEHOUSE_NAME = \"dbo\"          # <-- CHANGE THIS\n","SINGLE_WORKSPACE_NAME = None   # <-- or set to None to scan all\n","\n","# Validate lakehouse name\n","if not re.match(r'^[a-zA-Z0-9_]+$', LAKEHOUSE_NAME):\n","    raise ValueError(f\"Invalid lakehouse name: {LAKEHOUSE_NAME}\")\n","\n","EXTRACTION_TIMESTAMP = datetime.now()\n","REPORT_DATE = EXTRACTION_TIMESTAMP.strftime(\"%Y-%m-%d\")\n","start_time = time.time()\n","\n","# -----------------------------------\n","# Logging helpers\n","# -----------------------------------\n","def log(msg):\n","    print(msg, flush=True)\n","\n","def elapsed_min():\n","    return (time.time() - start_time) / 60\n","\n","# Heartbeat\n","import threading\n","heartbeat_running = True\n","def heartbeat():\n","    while heartbeat_running:\n","        time.sleep(10)\n","        print(f\"[Heartbeat] Still running… elapsed {elapsed_min():.2f} min\", flush=True)\n","\n","threading.Thread(target=heartbeat, daemon=True).start()\n","\n","# -----------------------------------\n","# Start banner\n","# -----------------------------------\n","log(\"=\"*80)\n","log(\"FABRIC MODEL METADATA EXTRACTION\")\n","log(f\"Started: {EXTRACTION_TIMESTAMP}\")\n","log(\"=\"*80)\n","\n","# ============================================\n","# AUTO-CREATE SCHEMA (LAKEHOUSE)\n","# ============================================\n","CATALOG = spark.sql(\"SELECT current_catalog()\").first()[0]\n","log(f\"Using catalog: {CATALOG}\")\n","\n","schema_name = f\"{CATALOG}.{LAKEHOUSE_NAME}\"\n","log(f\"Ensuring lakehouse schema exists: {schema_name}\")\n","\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n","log(f\"✓ Schema is ready: {schema_name}\\n\")\n","\n","# ==============================================================  \n","\n","\n","# ==============================================================  \n","# COLLECTIONS & SCHEMA TEMPLATES\n","# ==============================================================\n","# Each collection includes a template row that defines the schema.\n","# This ensures empty tables can be created with correct column structure.\n","\n","all_model_details = [{\n","    \"Type\": \"\",\n","    \"Table\": \"\",\n","    \"Name\": \"\",\n","    \"FormatString\": \"\",\n","    \"DisplayFolder\": \"\",\n","    \"Description\": \"\",\n","    \"IsHidden\": \"\",\n","    \"TableStorageMode\": \"\",\n","    \"Expression\": \"\",\n","    \"ModelAsOfDate\": \"\",\n","    \"ModelName\": \"\",\n","    \"ModelID\": \"\",\n","    \"WorkspaceName\": \"\",\n","    \"RelationshipFromTable\": \"\",\n","    \"RelationshipFromColumn\": \"\",\n","    \"RelationshipToTable\": \"\",\n","    \"RelationshipToColumn\": \"\",\n","    \"RelationshipStatus\": \"\",\n","    \"RelationshipFromCardinality\": \"\",\n","    \"RelationshipToCardinality\": \"\",\n","    \"RelationshipCrossFilteringBehavior\": \"\"\n","}]\n","\n","# Schema template for model dependencies\n","# Based on the Measure Dependency Extract Script.csx from:\n","# https://github.com/chris1642/Power-BI-Backup-Impact-Analysis-Governance-Solution\n","all_model_dependencies = [{\n","    \"ObjectName\": \"\",\n","    \"ObjectType\": \"\",\n","    \"DependsOn\": \"\",\n","    \"DependsOnType\": \"\",\n","    \"ModelAsOfDate\": \"\",\n","    \"ModelName\": \"\",\n","    \"ModelID\": \"\",\n","    \"WorkspaceName\": \"\"\n","}]\n","\n","# ==============================================================  \n","# HELPER FUNCTIONS\n","# ==============================================================\n","\n","def format_dax_object_name(table_name, object_name):\n","    \"\"\"Format a DAX object name as 'TableName'[ObjectName]\"\"\"\n","    return f\"'{table_name}'[{object_name}]\"\n","\n","def get_dependency_name(dep_obj):\n","    \"\"\"\n","    Get the formatted name of a dependency object based on its type.\n","    \n","    Args:\n","        dep_obj: The TOM object representing the dependency\n","    \n","    Returns:\n","        str: The formatted dependency name\n","    \"\"\"\n","    dep_type = str(dep_obj.ObjectType)\n","    \n","    if dep_type in [\"Measure\", \"Column\"]:\n","        return format_dax_object_name(dep_obj.Parent.Name, dep_obj.Name)\n","    elif dep_type == \"Table\":\n","        return f\"'{dep_obj.Name}'\"\n","    else:\n","        return dep_obj.Name\n","\n","# ==============================================================  \n","# GET WORKSPACES\n","# ==============================================================\n","\n","workspaces_df = fabric.list_workspaces()\n","\n","if SINGLE_WORKSPACE_NAME:\n","    workspaces_df = workspaces_df[workspaces_df[\"Name\"] == SINGLE_WORKSPACE_NAME]\n","    if workspaces_df.empty:\n","        raise ValueError(f\"Workspace '{SINGLE_WORKSPACE_NAME}' not found.\")\n","    log(f\"Filtering to workspace: {SINGLE_WORKSPACE_NAME}\")\n","\n","log(f\"Workspace count: {len(workspaces_df)}\")\n","log(\"\")\n","\n","# ==============================================================  \n","# MODEL METADATA EXTRACTION\n","# ==============================================================\n","\n","for ws_row in workspaces_df.itertuples(index=False):\n","    ws_name = ws_row.Name\n","    log(f\"\\nProcessing workspace: {ws_name} | Elapsed: {elapsed_min():.2f} min\")\n","\n","    try:\n","        datasets_df = fabric.list_datasets(workspace=ws_name)\n","        if datasets_df is None or datasets_df.empty:\n","            log(\"  No datasets found.\")\n","            continue\n","\n","        log(f\"  Datasets found: {len(datasets_df)}\")\n","\n","        for idx, row in datasets_df.iterrows():\n","            # Handle different possible column names\n","            model_name = row.get('Dataset Name') or row.get('Name') or row.get('Display Name', '')\n","            model_id = row.get('Dataset ID') or row.get('Id') or row.get('ID', '')\n","\n","            t0 = time.time()\n","            log(f\"\\n  [{idx}/{len(datasets_df)}] Extracting model: {model_name}\")\n","\n","            try:\n","                tom = TOMWrapper(dataset=model_name, workspace=ws_name, readonly=True)\n","\n","                # -------------------- Tables --------------------\n","                tables = tom.model.Tables\n","                log(f\"    Tables: {len(tables)}\")\n","                for t in tables:\n","                    storage_mode = \"\"\n","                    if t.Partitions.Count > 0:\n","                        # Access first partition through iteration since .NET collections don't support Python indexing\n","                        for p in t.Partitions:\n","                            if hasattr(p, 'Mode'):\n","                                storage_mode = p.Mode.ToString()\n","                            break  # Only get first partition\n","                    all_model_details.append({\n","                        \"Type\": \"Table\",\n","                        \"Table\": t.Name,\n","                        \"Name\": t.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": \"\",\n","                        \"Description\": \"\",\n","                        \"IsHidden\": str(t.IsHidden),\n","                        \"TableStorageMode\": storage_mode,\n","                        \"Expression\": \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Calculation Groups --------------------\n","                calc_groups = list(tom.all_calculation_groups())\n","                log(f\"    Calculation Groups: {len(calc_groups)}\")\n","                for cg in calc_groups:\n","                    all_model_details.append({\n","                        \"Type\": \"CalculationGroup\",\n","                        \"Table\": cg.Name,\n","                        \"Name\": cg.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": \"\",\n","                        \"Description\": cg.Description if cg.Description else \"\",\n","                        \"IsHidden\": str(cg.IsHidden),\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Calculation Items --------------------\n","                calc_items = list(tom.all_calculation_items())\n","                log(f\"    Calculation Items: {len(calc_items)}\")\n","                for ci in calc_items:\n","                    all_model_details.append({\n","                        \"Type\": \"CalculationItem\",\n","                        \"Table\": ci.CalculationGroup.Name,\n","                        \"Name\": ci.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": \"\",\n","                        \"Description\": ci.Description if ci.Description else \"\",\n","                        \"IsHidden\": \"\",\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": ci.Expression if ci.Expression else \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Columns --------------------\n","                columns = list(tom.all_columns())\n","                log(f\"    Columns: {len(columns)}\")\n","                for col in columns:\n","                    all_model_details.append({\n","                        \"Type\": \"Column\",\n","                        \"Table\": col.Table.Name,\n","                        \"Name\": col.Name,\n","                        \"FormatString\": col.FormatString if col.FormatString else \"\",\n","                        \"DisplayFolder\": col.DisplayFolder if col.DisplayFolder else \"\",\n","                        \"Description\": col.Description if col.Description else \"\",\n","                        \"IsHidden\": str(col.IsHidden),\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Calculated Columns --------------------\n","                calc_columns = list(tom.all_calculated_columns())\n","                log(f\"    Calculated Columns: {len(calc_columns)}\")\n","                for col in calc_columns:\n","                    all_model_details.append({\n","                        \"Type\": \"CalculatedColumn\",\n","                        \"Table\": col.Table.Name,\n","                        \"Name\": col.Name,\n","                        \"FormatString\": col.FormatString if col.FormatString else \"\",\n","                        \"DisplayFolder\": col.DisplayFolder if col.DisplayFolder else \"\",\n","                        \"Description\": col.Description if col.Description else \"\",\n","                        \"IsHidden\": str(col.IsHidden),\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": col.Expression if col.Expression else \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Measures --------------------\n","                measures = list(tom.all_measures())\n","                log(f\"    Measures: {len(measures)}\")\n","                for m in measures:\n","                    all_model_details.append({\n","                        \"Type\": \"Measure\",\n","                        \"Table\": m.Table.Name,\n","                        \"Name\": m.Name,\n","                        \"FormatString\": m.FormatString if m.FormatString else \"\",\n","                        \"DisplayFolder\": m.DisplayFolder if m.DisplayFolder else \"\",\n","                        \"Description\": m.Description if m.Description else \"\",\n","                        \"IsHidden\": str(m.IsHidden),\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": m.Expression if m.Expression else \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Hierarchies --------------------\n","                hierarchies = list(tom.all_hierarchies())\n","                log(f\"    Hierarchies: {len(hierarchies)}\")\n","                for h in hierarchies:\n","                    all_model_details.append({\n","                        \"Type\": \"Hierarchy\",\n","                        \"Table\": h.Table.Name,\n","                        \"Name\": h.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": h.DisplayFolder if h.DisplayFolder else \"\",\n","                        \"Description\": h.Description if h.Description else \"\",\n","                        \"IsHidden\": str(h.IsHidden),\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Levels --------------------\n","                levels = list(tom.all_levels())\n","                log(f\"    Levels: {len(levels)}\")\n","                for l in levels:\n","                    all_model_details.append({\n","                        \"Type\": \"Level\",\n","                        \"Table\": l.Hierarchy.Table.Name,\n","                        \"Name\": l.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": \"\",\n","                        \"Description\": l.Description if l.Description else \"\",\n","                        \"IsHidden\": \"\",\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": \"\",\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Partitions --------------------\n","                partitions = list(tom.all_partitions())\n","                log(f\"    Partitions: {len(partitions)}\")\n","                for p in partitions:\n","                    storage_mode = p.Mode.ToString() if hasattr(p, 'Mode') else \"\"\n","                    expression = \"\"\n","                    if hasattr(p, 'Source') and p.Source:\n","                        if hasattr(p.Source, 'Expression'):\n","                            expression = p.Source.Expression if p.Source.Expression else \"\"\n","                    all_model_details.append({\n","                        \"Type\": \"Partition\",\n","                        \"Table\": p.Table.Name,\n","                        \"Name\": p.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": \"\",\n","                        \"Description\": p.Description if p.Description else \"\",\n","                        \"IsHidden\": \"\",\n","                        \"TableStorageMode\": storage_mode,\n","                        \"Expression\": expression,\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": \"\",\n","                        \"RelationshipFromColumn\": \"\",\n","                        \"RelationshipToTable\": \"\",\n","                        \"RelationshipToColumn\": \"\",\n","                        \"RelationshipStatus\": \"\",\n","                        \"RelationshipFromCardinality\": \"\",\n","                        \"RelationshipToCardinality\": \"\",\n","                        \"RelationshipCrossFilteringBehavior\": \"\"\n","                    })\n","\n","                # -------------------- Relationships --------------------\n","                relationships = tom.model.Relationships\n","                log(f\"    Relationships: {len(relationships)}\")\n","                for r in relationships:\n","                    all_model_details.append({\n","                        \"Type\": \"Relationship\",\n","                        \"Table\": r.FromTable.Name,\n","                        \"Name\": r.FromColumn.Name,\n","                        \"FormatString\": \"\",\n","                        \"DisplayFolder\": \"\",\n","                        \"Description\": \"\",\n","                        \"IsHidden\": \"\",\n","                        \"TableStorageMode\": \"\",\n","                        \"Expression\": r.Name if r.Name else \"\",  # Matches C# script structure\n","                        \"ModelAsOfDate\": REPORT_DATE,\n","                        \"ModelName\": model_name,\n","                        \"ModelID\": model_id,\n","                        \"WorkspaceName\": ws_name,\n","                        \"RelationshipFromTable\": r.FromTable.Name,\n","                        \"RelationshipFromColumn\": r.FromColumn.Name,\n","                        \"RelationshipToTable\": r.ToTable.Name,\n","                        \"RelationshipToColumn\": r.ToColumn.Name,\n","                        \"RelationshipStatus\": str(r.IsActive),\n","                        \"RelationshipFromCardinality\": r.FromCardinality.ToString(),\n","                        \"RelationshipToCardinality\": r.ToCardinality.ToString(),\n","                        \"RelationshipCrossFilteringBehavior\": r.CrossFilteringBehavior.ToString()\n","                    })\n","\n","                # -------------------- Model Dependencies --------------------\n","                # Uses TOMWrapper.depends_on method documented at:\n","                # https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.tom.html#sempy_labs.tom.TOMWrapper.depends_on\n","                try:\n","                    dependencies_df = get_model_calc_dependencies(\n","                        dataset=model_name,\n","                        workspace=ws_name\n","                    )\n","                    \n","                    if dependencies_df is not None and not dependencies_df.empty:\n","                        dep_count_before = len(all_model_dependencies)\n","                        \n","                        # Measure Dependencies\n","                        for m in measures:\n","                            try:\n","                                for dep_obj in tom.depends_on(object=m, dependencies=dependencies_df):\n","                                    all_model_dependencies.append({\n","                                        \"ObjectName\": m.Name,\n","                                        \"ObjectType\": \"Measure\",\n","                                        \"DependsOn\": get_dependency_name(dep_obj),\n","                                        \"DependsOnType\": str(dep_obj.ObjectType),\n","                                        \"ModelAsOfDate\": REPORT_DATE,\n","                                        \"ModelName\": model_name,\n","                                        \"ModelID\": model_id,\n","                                        \"WorkspaceName\": ws_name\n","                                    })\n","                            except Exception as e:\n","                                log(f\"      Warning: Could not get dependencies for measure {m.Name}: {e}\")\n","\n","                        # Calculated Column Dependencies\n","                        for col in calc_columns:\n","                            try:\n","                                for dep_obj in tom.depends_on(object=col, dependencies=dependencies_df):\n","                                    all_model_dependencies.append({\n","                                        \"ObjectName\": col.Name,\n","                                        \"ObjectType\": \"CalculatedColumn\",\n","                                        \"DependsOn\": get_dependency_name(dep_obj),\n","                                        \"DependsOnType\": str(dep_obj.ObjectType),\n","                                        \"ModelAsOfDate\": REPORT_DATE,\n","                                        \"ModelName\": model_name,\n","                                        \"ModelID\": model_id,\n","                                        \"WorkspaceName\": ws_name\n","                                    })\n","                            except Exception as e:\n","                                log(f\"      Warning: Could not get dependencies for calculated column {col.Name}: {e}\")\n","\n","                        # Calculation Item Dependencies\n","                        for ci in calc_items:\n","                            try:\n","                                for dep_obj in tom.depends_on(object=ci, dependencies=dependencies_df):\n","                                    all_model_dependencies.append({\n","                                        \"ObjectName\": ci.Name,\n","                                        \"ObjectType\": \"CalculationItem\",\n","                                        \"DependsOn\": get_dependency_name(dep_obj),\n","                                        \"DependsOnType\": str(dep_obj.ObjectType),\n","                                        \"ModelAsOfDate\": REPORT_DATE,\n","                                        \"ModelName\": model_name,\n","                                        \"ModelID\": model_id,\n","                                        \"WorkspaceName\": ws_name\n","                                    })\n","                            except Exception as e:\n","                                log(f\"      Warning: Could not get dependencies for calculation item {ci.Name}: {e}\")\n","                        \n","                        dep_count = len(all_model_dependencies) - dep_count_before\n","                        log(f\"    Dependencies extracted: {dep_count}\")\n","                    else:\n","                        log(f\"    No dependencies found\")\n","                except Exception as e:\n","                    log(f\"    Warning: Could not extract dependencies: {e}\")\n","\n","            except Exception as e:\n","                log(f\"    ERROR extracting {model_name}: {e}\")\n","\n","            log(f\"  → Finished {model_name} in {time.time() - t0:.1f} sec \"\n","                f\"(Total: {elapsed_min():.2f} min)\")\n","\n","    except Exception as e:\n","        log(f\"ERROR accessing workspace {ws_name}: {e}\")\n","\n","# ==============================================================  \n","# WRITE TO LAKEHOUSE\n","# ==============================================================\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"Writing output to Lakehouse\")\n","log(\"=\"*80)\n","\n","def write_table(data, name):\n","    \"\"\"\n","    Write data to a Delta table. Schema is inferred from the first row (template).\n","    Creates empty table with schema if only template row exists.\n","    \n","    Args:\n","        data: List of dictionaries containing the data (first row is schema template)\n","        name: Name of the table\n","    \"\"\"\n","    full_name = f\"{CATALOG}.{LAKEHOUSE_NAME}.{name}\"\n","    \n","    # Check if we only have the template row (length 1 means just the schema template)\n","    if len(data) == 1:\n","        log(f\"⚠ No data for {name}, creating empty table with schema\")\n","        # Use template to create empty DataFrame with correct schema\n","        df = spark.createDataFrame(pd.DataFrame(data))\n","        # Filter out the template row to create truly empty table\n","        empty_df = df.filter(\"1=0\")\n","        empty_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","        log(f\"✓ Created empty table: {full_name}\\n\")\n","        return\n","\n","    # Skip the template row (first row) and create DataFrame with actual data\n","    pandas_df = pd.DataFrame(data)\n","    actual_df = spark.createDataFrame(pandas_df.iloc[1:])\n","    count = actual_df.count()\n","\n","    log(f\"Writing {count} rows → {full_name}\")\n","\n","    actual_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","\n","    log(f\"✓ Wrote table: {full_name}\\n\")\n","\n","write_table(all_model_details, \"ModelDetail\")\n","write_table(all_model_dependencies, \"ModelDependencies\")\n","\n","# ==============================================================  \n","# END\n","# ==============================================================\n","\n","heartbeat_running = False\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"PROCESS COMPLETE\")\n","log(f\"Finished at: {datetime.now()}\")\n","log(f\"Total runtime: {elapsed_min():.2f} minutes\")\n","log(\"=\"*80)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4bddf5be-3092-4037-a288-9b836b44d35c"},{"cell_type":"code","source":["# ================================\n","# FABRIC DATAFLOW DETAIL EXTRACTOR\n","# WITH AUTO-SCHEMA CREATION\n","# ================================\n","#\n","# This notebook extracts dataflow detail metadata (queries/entities)\n","# using Fabric REST APIs, similar to the PowerShell script from:\n","# https://github.com/chris1642/Power-BI-Backup-Impact-Analysis-Governance-Solution\n","#\n","# EXTRACTED DATA (written to lakehouse tables):\n","# 1. DataflowDetail - dataflow queries with M expressions\n","#\n","# Column names match the PowerShell script output:\n","# - Dataflow ID\n","# - Dataflow Name\n","# - Query Name\n","# - Query (M expression)\n","# - Report Date\n","# - Workspace Name - Dataflow Name\n","# ================================\n","\n","# %pip install semantic-link-labs --quiet\n","\n","import time, re, pandas as pd, json, base64\n","from datetime import datetime\n","import sempy.fabric as fabric\n","from sempy.fabric import FabricRestClient\n","\n","# -----------------------------------\n","# CONFIG\n","# -----------------------------------\n","LAKEHOUSE_NAME = \"dbo\"          # <-- CHANGE THIS\n","SINGLE_WORKSPACE_NAME = None   # <-- or set to None to scan all\n","\n","# Validate lakehouse name\n","if not re.match(r'^[a-zA-Z0-9_]+$', LAKEHOUSE_NAME):\n","    raise ValueError(f\"Invalid lakehouse name: {LAKEHOUSE_NAME}\")\n","\n","EXTRACTION_TIMESTAMP = datetime.now()\n","REPORT_DATE = EXTRACTION_TIMESTAMP.strftime(\"%Y-%m-%d\")\n","start_time = time.time()\n","\n","# -----------------------------------\n","# Logging helpers\n","# -----------------------------------\n","def log(msg):\n","    print(msg, flush=True)\n","\n","def elapsed_min():\n","    return (time.time() - start_time) / 60\n","\n","# Heartbeat\n","import threading\n","heartbeat_running = True\n","def heartbeat():\n","    while heartbeat_running:\n","        time.sleep(10)\n","        print(f\"[Heartbeat] Still running… elapsed {elapsed_min():.2f} min\", flush=True)\n","\n","threading.Thread(target=heartbeat, daemon=True).start()\n","\n","# -----------------------------------\n","# Start banner\n","# -----------------------------------\n","log(\"=\"*80)\n","log(\"FABRIC DATAFLOW DETAIL EXTRACTION\")\n","log(f\"Started: {EXTRACTION_TIMESTAMP}\")\n","log(\"=\"*80)\n","\n","# ============================================\n","# AUTO-CREATE SCHEMA (LAKEHOUSE)\n","# ============================================\n","CATALOG = spark.sql(\"SELECT current_catalog()\").first()[0]\n","log(f\"Using catalog: {CATALOG}\")\n","\n","schema_name = f\"{CATALOG}.{LAKEHOUSE_NAME}\"\n","log(f\"Ensuring lakehouse schema exists: {schema_name}\")\n","\n","spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n","log(f\"✓ Schema is ready: {schema_name}\\n\")\n","\n","# ==============================================================  \n","\n","\n","# ==============================================================  \n","# COLLECTIONS & SCHEMA TEMPLATES\n","# ==============================================================\n","# Each collection includes a template row that defines the schema.\n","# This ensures empty tables can be created with correct column structure.\n","# Schema matches the PowerShell script output from Final PS Script.txt\n","\n","all_dataflow_details = [{\n","    \"DataflowId\": \"\",\n","    \"DataflowName\": \"\",\n","    \"QueryName\": \"\",\n","    \"Query\": \"\",\n","    \"ReportDate\": \"\",\n","    \"WorkspaceName\": \"\",\n","    \"WorkspaceNameDataflowName\": \"\"\n","}]\n","\n","# ==============================================================  \n","# HELPER FUNCTIONS\n","# ==============================================================\n","\n","def clean_name(name):\n","    \"\"\"Clean up names for file/display purposes (matches PowerShell script pattern)\"\"\"\n","    clean = name.replace('[', '(').replace(']', ')')\n","    clean = re.sub(r'[^a-zA-Z0-9\\(\\)&,.\\- ]', ' ', clean)\n","    return clean.strip()\n","\n","def parse_power_query_document(document_content, dataflow_id, dataflow_name, workspace_name, report_date):\n","    \"\"\"\n","    Parse Power Query document content to extract queries.\n","    Handles both Gen1 and Gen2 dataflow document formats.\n","    \n","    Args:\n","        document_content: The Power Query M document content\n","        dataflow_id: Dataflow ID\n","        dataflow_name: Dataflow name\n","        workspace_name: Workspace name\n","        report_date: Report date\n","    \n","    Returns:\n","        List of query dictionaries\n","    \"\"\"\n","    queries = []\n","    \n","    clean_workspace_name = clean_name(workspace_name)\n","    clean_dataflow_name = clean_name(dataflow_name)\n","    workspace_dataflow_name = f\"{clean_workspace_name} ~ {clean_dataflow_name}\"\n","    \n","    # Unescape content if needed (Gen1 dataflows have escaped content)\n","    document_content = document_content.replace('\\\\r\\\\n', '\\n').replace('\\\\n', '\\n')\n","    document_content = document_content.replace('\\\\\"', '\"')\n","    \n","    # Split by \"section Section1;\" to get the queries section\n","    sections = document_content.split('section Section1;', 1)\n","    \n","    if len(sections) < 2:\n","        return queries\n","    \n","    queries_section = sections[1]\n","    \n","    # Use regex to find all queries in Power Query M document format\n","    # Pattern breakdown:\n","    #   (?s)                           - DOTALL mode: dot matches newlines\n","    #   (?:\\[[^\\]]*\\]\\s*)?             - Optional metadata annotations like [IsEnabled=false]\n","    #   shared\\s+                       - \"shared\" keyword followed by whitespace\n","    #   (?:#\"(.*?)\"|([A-Za-z_]\\w*))    - Query name: either #\"quoted name\" (group 1) or unquoted identifier (group 2)\n","    #   \\s*=\\s*                         - Assignment operator with optional whitespace\n","    #   (.*?)                           - Query expression (group 3) - non-greedy capture\n","    #   (?=...)                         - Lookahead: stop before next \"shared\" keyword or end of string\n","    # Supports both: shared QueryName = ... and shared #\"Query Name With Spaces\" = ...\n","    pattern = r'(?s)(?:\\[[^\\]]*\\]\\s*)?shared\\s+(?:#\"(.*?)\"|([A-Za-z_]\\w*))\\s*=\\s*(.*?)(?=(?:\\[[^\\]]*\\]\\s*)?shared\\s+(?:#\"(?:.*?)\"|[A-Za-z_]\\w*)\\s*=|$)'\n","    matches = re.findall(pattern, queries_section)\n","    \n","    for match in matches:\n","        # Group 0 = hash-quoted name, Group 1 = unquoted name, Group 2 = expression\n","        query_name = match[0] if match[0] else match[1]\n","        query_expression = match[2].strip()\n","        \n","        # Remove trailing semicolons\n","        query_expression = re.sub(r';\\s*$', '', query_expression).strip()\n","        \n","        # Skip if empty\n","        if not query_name or not query_expression:\n","            continue\n","        \n","        queries.append({\n","            \"DataflowId\": dataflow_id,\n","            \"DataflowName\": dataflow_name,\n","            \"QueryName\": query_name,\n","            \"Query\": query_expression,\n","            \"ReportDate\": report_date,\n","            \"WorkspaceName\": workspace_name,\n","            \"WorkspaceNameDataflowName\": workspace_dataflow_name\n","        })\n","    \n","    return queries\n","\n","def extract_gen2_dataflow(client, workspace_id, dataflow_id, dataflow_name, workspace_name, report_date):\n","    \"\"\"\n","    Extract Gen2 (Fabric) dataflow definition using getDefinition API.\n","    \n","    Args:\n","        client: FabricRestClient instance\n","        workspace_id: Workspace ID\n","        dataflow_id: Dataflow ID\n","        dataflow_name: Dataflow name\n","        workspace_name: Workspace name\n","        report_date: Report date\n","    \n","    Returns:\n","        List of query dictionaries\n","    \"\"\"\n","    queries = []\n","    \n","    try:\n","        # Use Fabric API to get dataflow definition\n","        endpoint = f\"v1/workspaces/{workspace_id}/dataflows/{dataflow_id}/getDefinition\"\n","        response = client.post(endpoint, json={})\n","        \n","        if response.status_code != 200:\n","            return queries\n","        \n","        response_data = response.json()\n","        \n","        if not response_data.get('definition', {}).get('parts'):\n","            return queries\n","        \n","        # Find the .pq file in the parts\n","        for part in response_data['definition']['parts']:\n","            file_path = part.get('path', '')\n","            payload_type = part.get('payloadType', '')\n","            payload = part.get('payload', '')\n","            \n","            if file_path.endswith('.pq') and payload_type == 'InlineBase64':\n","                # Decode Base64 content\n","                try:\n","                    decoded_bytes = base64.b64decode(payload)\n","                    pq_content = decoded_bytes.decode('utf-8')\n","                    \n","                    # Parse the Power Query document\n","                    queries = parse_power_query_document(\n","                        pq_content,\n","                        dataflow_id,\n","                        dataflow_name,\n","                        workspace_name,\n","                        report_date\n","                    )\n","                    break\n","                except Exception as e:\n","                    log(f\"      Error decoding Gen2 dataflow content: {e}\")\n","    \n","    except Exception as e:\n","        log(f\"    Could not extract Gen2 dataflow {dataflow_name}: {e}\")\n","    \n","    return queries\n","\n","def extract_gen1_dataflow(client, workspace_id, dataflow_id, dataflow_name, workspace_name, report_date):\n","    \"\"\"\n","    Extract Gen1 (Power BI) dataflow definition using REST API.\n","    \n","    Args:\n","        client: FabricRestClient instance\n","        workspace_id: Workspace ID\n","        dataflow_id: Dataflow ID\n","        dataflow_name: Dataflow name\n","        workspace_name: Workspace name\n","        report_date: Report date\n","    \n","    Returns:\n","        List of query dictionaries\n","    \"\"\"\n","    queries = []\n","    \n","    try:\n","        # Use Power BI API to get dataflow definition\n","        api_url = f\"v1.0/myorg/groups/{workspace_id}/dataflows/{dataflow_id}\"\n","        response = client.get(api_url)\n","        \n","        if response.status_code != 200:\n","            return queries\n","        \n","        dataflow_json = response.json()\n","        \n","        # Check for pbi:mashup document content\n","        if 'pbi:mashup' not in dataflow_json or 'document' not in dataflow_json['pbi:mashup']:\n","            return queries\n","        \n","        document_content = dataflow_json['pbi:mashup']['document']\n","        \n","        # Parse the Power Query document\n","        queries = parse_power_query_document(\n","            document_content,\n","            dataflow_id,\n","            dataflow_name,\n","            workspace_name,\n","            report_date\n","        )\n","    \n","    except Exception as e:\n","        log(f\"    Could not extract Gen1 dataflow {dataflow_name}: {e}\")\n","    \n","    return queries\n","\n","# ==============================================================  \n","# GET WORKSPACES\n","# ==============================================================\n","\n","workspaces_df = fabric.list_workspaces()\n","\n","if SINGLE_WORKSPACE_NAME:\n","    workspaces_df = workspaces_df[workspaces_df[\"Name\"] == SINGLE_WORKSPACE_NAME]\n","    if workspaces_df.empty:\n","        raise ValueError(f\"Workspace '{SINGLE_WORKSPACE_NAME}' not found.\")\n","    log(f\"Filtering to workspace: {SINGLE_WORKSPACE_NAME}\")\n","\n","log(f\"Workspace count: {len(workspaces_df)}\")\n","log(\"\")\n","\n","# Create REST client instance\n","client = FabricRestClient()\n","\n","# ==============================================================  \n","# DATAFLOW DETAIL EXTRACTION\n","# ==============================================================\n","\n","for ws_row in workspaces_df.itertuples(index=False):\n","    ws_name = ws_row.Name\n","    ws_id = ws_row.Id\n","    log(f\"\\nProcessing workspace: {ws_name} | Elapsed: {elapsed_min():.2f} min\")\n","\n","    # -------------------- Gen1 Dataflows (Power BI API) --------------------\n","    try:\n","        log(f\"  Fetching Gen1 dataflows...\")\n","        dataflows_url = f\"v1.0/myorg/groups/{ws_id}/dataflows\"\n","        response = client.get(dataflows_url)\n","        \n","        if response.status_code == 200:\n","            dataflows = response.json().get('value', [])\n","            log(f\"  Gen1 Dataflows found: {len(dataflows)}\")\n","            \n","            for dataflow in dataflows:\n","                dataflow_id = dataflow.get('objectId', '')\n","                dataflow_name = dataflow.get('name', '')\n","                \n","                log(f\"    Extracting: {dataflow_name}\")\n","                \n","                queries = extract_gen1_dataflow(\n","                    client,\n","                    ws_id,\n","                    dataflow_id,\n","                    dataflow_name,\n","                    ws_name,\n","                    REPORT_DATE\n","                )\n","                \n","                if queries:\n","                    all_dataflow_details.extend(queries)\n","                    log(f\"      Queries extracted: {len(queries)}\")\n","                else:\n","                    log(f\"      No queries found\")\n","        else:\n","            log(f\"  No Gen1 dataflows found\")\n","    except Exception as e:\n","        log(f\"  ERROR fetching Gen1 dataflows: {e}\")\n","\n","    # -------------------- Gen2 Dataflows (Fabric API) --------------------\n","    try:\n","        log(f\"  Fetching Gen2 dataflows...\")\n","        items_url = f\"v1/workspaces/{ws_id}/items\"\n","        response = client.get(items_url)\n","        \n","        if response.status_code == 200:\n","            items = response.json().get('value', [])\n","            gen2_dataflows = [item for item in items if item.get('type') == 'Dataflow']\n","            \n","            log(f\"  Gen2 Dataflows found: {len(gen2_dataflows)}\")\n","            \n","            for dataflow in gen2_dataflows:\n","                dataflow_id = dataflow.get('id', '')\n","                dataflow_name = dataflow.get('displayName', '')\n","                \n","                log(f\"    Extracting: {dataflow_name}\")\n","                \n","                queries = extract_gen2_dataflow(\n","                    client,\n","                    ws_id,\n","                    dataflow_id,\n","                    dataflow_name,\n","                    ws_name,\n","                    REPORT_DATE\n","                )\n","                \n","                if queries:\n","                    all_dataflow_details.extend(queries)\n","                    log(f\"      Queries extracted: {len(queries)}\")\n","                else:\n","                    log(f\"      No queries found\")\n","        else:\n","            log(f\"  No Gen2 dataflows found\")\n","    except Exception as e:\n","        log(f\"  ERROR fetching Gen2 dataflows: {e}\")\n","    \n","    log(f\"✓ Finished workspace: {ws_name}\")\n","\n","# ==============================================================  \n","# WRITE TO LAKEHOUSE\n","# ==============================================================\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"Writing output to Lakehouse\")\n","log(\"=\"*80)\n","\n","def write_table(data, name):\n","    \"\"\"\n","    Write data to a Delta table. Schema is inferred from the first row (template).\n","    Creates empty table with schema if only template row exists.\n","    \n","    Args:\n","        data: List of dictionaries containing the data (first row is schema template)\n","        name: Name of the table\n","    \"\"\"\n","    full_name = f\"{CATALOG}.{LAKEHOUSE_NAME}.{name}\"\n","    \n","    # Check if we only have the template row (length 1 means just the schema template)\n","    if len(data) == 1:\n","        log(f\"⚠ No data for {name}, creating empty table with schema\")\n","        # Use template to create empty DataFrame with correct schema\n","        df = spark.createDataFrame(pd.DataFrame(data))\n","        # Filter out the template row to create truly empty table\n","        empty_df = df.filter(\"1=0\")\n","        empty_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","        log(f\"✓ Created empty table: {full_name}\\n\")\n","        return\n","\n","    # Skip the template row (first row) and create DataFrame with actual data\n","    pandas_df = pd.DataFrame(data)\n","    actual_df = spark.createDataFrame(pandas_df.iloc[1:])\n","    count = actual_df.count()\n","\n","    log(f\"Writing {count} rows → {full_name}\")\n","\n","    actual_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").saveAsTable(full_name)\n","\n","    log(f\"✓ Wrote table: {full_name}\\n\")\n","\n","write_table(all_dataflow_details, \"DataflowDetail\")\n","\n","# ==============================================================  \n","# END\n","# ==============================================================\n","\n","heartbeat_running = False\n","\n","log(\"\\n\" + \"=\"*80)\n","log(\"PROCESS COMPLETE\")\n","log(f\"Finished at: {datetime.now()}\")\n","log(f\"Total runtime: {elapsed_min():.2f} minutes\")\n","log(\"=\"*80)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad55dee3-870f-454e-80d7-5a38b5be6547"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"28800000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b11e2ae9-111f-4c07-a727-d6dc986e35fb"}],"default_lakehouse":"b11e2ae9-111f-4c07-a727-d6dc986e35fb","default_lakehouse_name":"GovernanceLakehouse","default_lakehouse_workspace_id":"bda551b6-876a-4868-a55b-1b10271f4002"},"environment":{"environmentId":"c30767e1-b409-41d0-a0e7-898261bfddee","workspaceId":"bda551b6-876a-4868-a55b-1b10271f4002"}}},"nbformat":4,"nbformat_minor":5}