# ================================
# FABRIC MODEL DEPENDENCIES EXTRACTOR (TOMWrapper)
# WITH AUTO-SCHEMA CREATION
# ================================
#
# This notebook extracts model dependencies (measure and calculated column
# dependencies) using the semantic-link-labs TOMWrapper and get_model_calc_dependencies.
# 
# Based on the Measure Dependency Extract Script.csx from:
# https://github.com/chris1642/Power-BI-Backup-Impact-Analysis-Governance-Solution
#
# Uses TOMWrapper.depends_on method documented at:
# https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.tom.html#sempy_labs.tom.TOMWrapper.depends_on
#
# EXTRACTED DATA (written to lakehouse tables):
# 1. ModelDependencies - measure and calculated column dependencies
# ================================

%pip install semantic-link-labs --quiet

import time, re, pandas as pd
from datetime import datetime
import sempy.fabric as fabric
from sempy_labs.tom import TOMWrapper
from sempy_labs._model_dependencies import get_model_calc_dependencies

# -----------------------------------
# CONFIG
# -----------------------------------
LAKEHOUSE_NAME = "dbo"          # <-- CHANGE THIS
SINGLE_WORKSPACE_NAME = None   # <-- or set to None to scan all

# Validate lakehouse name
if not re.match(r'^[a-zA-Z0-9_]+$', LAKEHOUSE_NAME):
    raise ValueError(f"Invalid lakehouse name: {LAKEHOUSE_NAME}")

EXTRACTION_TIMESTAMP = datetime.now()
REPORT_DATE = EXTRACTION_TIMESTAMP.strftime("%Y-%m-%d")
start_time = time.time()

# -----------------------------------
# Logging helpers
# -----------------------------------
def log(msg):
    print(msg, flush=True)

def elapsed_min():
    return (time.time() - start_time) / 60

# Heartbeat
import threading
heartbeat_running = True
def heartbeat():
    while heartbeat_running:
        time.sleep(10)
        print(f"[Heartbeat] Still running… elapsed {elapsed_min():.2f} min", flush=True)

threading.Thread(target=heartbeat, daemon=True).start()

# -----------------------------------
# Start banner
# -----------------------------------
log("="*80)
log("FABRIC MODEL DEPENDENCIES EXTRACTION")
log(f"Started: {EXTRACTION_TIMESTAMP}")
log("="*80)

# ============================================
# AUTO-CREATE SCHEMA (LAKEHOUSE)
# ============================================
CATALOG = spark.sql("SELECT current_catalog()").first()[0]
log(f"Using catalog: {CATALOG}")

schema_name = f"{CATALOG}.{LAKEHOUSE_NAME}"
log(f"Ensuring lakehouse schema exists: {schema_name}")

spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
log(f"✓ Schema is ready: {schema_name}\n")

# ==============================================================  
# COLLECTIONS & SCHEMA TEMPLATES
# ==============================================================
# Each collection includes a template row that defines the schema.
# This ensures empty tables can be created with correct column structure.
# Schema matches the C# Measure Dependency Extract Script.csx

all_model_dependencies = [{
    "ObjectName": "",
    "ObjectType": "",
    "DependsOn": "",
    "DependsOnType": "",
    "ModelAsOfDate": "",
    "ModelName": "",
    "ModelID": ""
}]

# ==============================================================  
# GET WORKSPACES
# ==============================================================

workspaces_df = fabric.list_workspaces()

if SINGLE_WORKSPACE_NAME:
    workspaces_df = workspaces_df[workspaces_df["Name"] == SINGLE_WORKSPACE_NAME]
    if workspaces_df.empty:
        raise ValueError(f"Workspace '{SINGLE_WORKSPACE_NAME}' not found.")
    log(f"Filtering to workspace: {SINGLE_WORKSPACE_NAME}")

log(f"Workspace count: {len(workspaces_df)}")
log("")

# ==============================================================  
# HELPER FUNCTIONS
# ==============================================================

def format_dax_object_name(table_name, object_name):
    """Format a DAX object name as 'TableName'[ObjectName]"""
    return f"'{table_name}'[{object_name}]"

def get_dependency_name(dep_obj):
    """
    Get the formatted name of a dependency object based on its type.
    
    Args:
        dep_obj: The TOM object representing the dependency
    
    Returns:
        str: The formatted dependency name
    """
    dep_type = str(dep_obj.ObjectType)
    
    if dep_type in ["Measure", "Column"]:
        return format_dax_object_name(dep_obj.Parent.Name, dep_obj.Name)
    elif dep_type == "Table":
        return f"'{dep_obj.Name}'"
    else:
        return dep_obj.Name

# ==============================================================  
# MODEL DEPENDENCIES EXTRACTION
# ==============================================================

for ws_row in workspaces_df.itertuples(index=False):
    ws_name = ws_row.Name
    log(f"\nProcessing workspace: {ws_name} | Elapsed: {elapsed_min():.2f} min")

    try:
        datasets_df = fabric.list_datasets(workspace=ws_name)
        if datasets_df is None or datasets_df.empty:
            log("  No datasets found.")
            continue

        log(f"  Datasets found: {len(datasets_df)}")

        for idx, row in datasets_df.iterrows():
            # Handle different possible column names
            model_name = row.get('Dataset Name') or row.get('Name') or row.get('Display Name', '')
            model_id = row.get('Dataset ID') or row.get('Id') or row.get('ID', '')

            t0 = time.time()
            log(f"\n  [{idx}/{len(datasets_df)}] Extracting dependencies: {model_name}")

            try:
                # Get the model calculation dependencies using the sempy_labs function
                dependencies_df = get_model_calc_dependencies(
                    dataset=model_name,
                    workspace=ws_name
                )
                
                if dependencies_df is None or dependencies_df.empty:
                    log(f"    No dependencies found for {model_name}")
                    continue

                tom = TOMWrapper(dataset=model_name, workspace=ws_name, readonly=True)

                # -------------------- Measure Dependencies --------------------
                measures = list(tom.all_measures())
                log(f"    Measures: {len(measures)}")
                
                for m in measures:
                    try:
                        # Get objects this measure depends on
                        for dep_obj in tom.depends_on(object=m, dependencies=dependencies_df):
                            all_model_dependencies.append({
                                "ObjectName": m.Name,
                                "ObjectType": "Measure",
                                "DependsOn": get_dependency_name(dep_obj),
                                "DependsOnType": str(dep_obj.ObjectType),
                                "ModelAsOfDate": REPORT_DATE,
                                "ModelName": model_name,
                                "ModelID": model_id
                            })
                    except Exception as e:
                        log(f"      Warning: Could not get dependencies for measure {m.Name}: {e}")

                # -------------------- Calculated Column Dependencies --------------------
                calc_columns = list(tom.all_calculated_columns())
                log(f"    Calculated Columns: {len(calc_columns)}")
                
                for col in calc_columns:
                    try:
                        # Get objects this calculated column depends on
                        for dep_obj in tom.depends_on(object=col, dependencies=dependencies_df):
                            all_model_dependencies.append({
                                "ObjectName": col.Name,
                                "ObjectType": "CalculatedColumn",
                                "DependsOn": get_dependency_name(dep_obj),
                                "DependsOnType": str(dep_obj.ObjectType),
                                "ModelAsOfDate": REPORT_DATE,
                                "ModelName": model_name,
                                "ModelID": model_id
                            })
                    except Exception as e:
                        log(f"      Warning: Could not get dependencies for calculated column {col.Name}: {e}")

                # -------------------- Calculation Item Dependencies --------------------
                calc_items = list(tom.all_calculation_items())
                log(f"    Calculation Items: {len(calc_items)}")
                
                for ci in calc_items:
                    try:
                        # Get objects this calculation item depends on
                        for dep_obj in tom.depends_on(object=ci, dependencies=dependencies_df):
                            all_model_dependencies.append({
                                "ObjectName": ci.Name,
                                "ObjectType": "CalculationItem",
                                "DependsOn": get_dependency_name(dep_obj),
                                "DependsOnType": str(dep_obj.ObjectType),
                                "ModelAsOfDate": REPORT_DATE,
                                "ModelName": model_name,
                                "ModelID": model_id
                            })
                    except Exception as e:
                        log(f"      Warning: Could not get dependencies for calculation item {ci.Name}: {e}")

            except Exception as e:
                log(f"    ERROR extracting {model_name}: {e}")

            log(f"  → Finished {model_name} in {time.time() - t0:.1f} sec "
                f"(Total: {elapsed_min():.2f} min)")

    except Exception as e:
        log(f"ERROR accessing workspace {ws_name}: {e}")

# ==============================================================  
# WRITE TO LAKEHOUSE
# ==============================================================

log("\n" + "="*80)
log("Writing output to Lakehouse")
log("="*80)

def write_table(data, name):
    """
    Write data to a Delta table. Schema is inferred from the first row (template).
    Creates empty table with schema if only template row exists.
    
    Args:
        data: List of dictionaries containing the data (first row is schema template)
        name: Name of the table
    """
    full_name = f"{CATALOG}.{LAKEHOUSE_NAME}.{name}"
    
    # Check if we only have the template row (length 1 means just the schema template)
    if len(data) == 1:
        log(f"⚠ No data for {name}, creating empty table with schema")
        # Use template to create empty DataFrame with correct schema
        df = spark.createDataFrame(pd.DataFrame(data))
        # Filter out the template row to create truly empty table
        empty_df = df.filter("1=0")
        empty_df.write.mode("overwrite").format("delta").saveAsTable(full_name)
        log(f"✓ Created empty table: {full_name}\n")
        return

    # Skip the template row (first row) and create DataFrame with actual data
    pandas_df = pd.DataFrame(data)
    actual_df = spark.createDataFrame(pandas_df.iloc[1:])
    count = actual_df.count()

    log(f"Writing {count} rows → {full_name}")

    actual_df.write.mode("overwrite").format("delta").saveAsTable(full_name)

    log(f"✓ Wrote table: {full_name}\n")

write_table(all_model_dependencies, "ModelDependencies")

# ==============================================================  
# END
# ==============================================================

heartbeat_running = False

log("\n" + "="*80)
log("PROCESS COMPLETE")
log(f"Finished at: {datetime.now()}")
log(f"Total runtime: {elapsed_min():.2f} minutes")
log("="*80)
